{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKKa+qY+fjsqXfcUCzaVXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sowmya74/Apple_Data_Analysis/blob/main/WebScraping_Apple_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def clean_filename(filename):\n",
        "    return re.sub(r'[^\\w\\-_\\. ]', '_', filename)\n",
        "\n",
        "def scrape_tables(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Referer': 'https://www.google.com/',\n",
        "        'DNT': '1',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        session = requests.Session()\n",
        "        response = session.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        time.sleep(2)  # Add a delay to avoid overwhelming the server\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    all_tables = soup.find_all('table')\n",
        "    print(f\"Total tables found: {len(all_tables)}\")\n",
        "\n",
        "    tables = soup.find_all('table', {'class': ['has-fixed-layout', 'table-primary']})\n",
        "    print(f\"Tables with specified classes: {len(tables)}\")\n",
        "\n",
        "    if not tables:\n",
        "        print(\"No tables with specified classes found. Trying to scrape all tables.\")\n",
        "        tables = all_tables\n",
        "\n",
        "    csv_files = []\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        header = table.find_previous(['h2', 'h3', 'h4'])\n",
        "        if header:\n",
        "            filename = clean_filename(header.text.strip()) + '.csv'\n",
        "        else:\n",
        "            filename = f'table_{i+1}.csv'\n",
        "\n",
        "        headers = []\n",
        "        data = []\n",
        "\n",
        "        rows = table.find_all('tr')\n",
        "        if not rows:\n",
        "            print(f\"No rows found in table {i+1}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        for row in rows:\n",
        "            cols = row.find_all(['th', 'td'])\n",
        "            if cols:\n",
        "                if not headers:\n",
        "                    headers = [col.text.strip() for col in cols]\n",
        "                else:\n",
        "                    data.append([col.text.strip() for col in cols])\n",
        "\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "            writer.writerows(data)\n",
        "\n",
        "        csv_files.append(filename)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "    return csv_files\n",
        "\n",
        "def create_zip(csv_files, zip_filename):\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        for file in csv_files:\n",
        "            # Check if the file exists before attempting to add it to the zip\n",
        "            if os.path.exists(file):\n",
        "                zipf.write(file, os.path.basename(file))\n",
        "                # Remove the file after adding it to the zip\n",
        "                os.remove(file)\n",
        "            else:\n",
        "                print(f\"Warning: File {file} not found and will be skipped.\")\n",
        "    print(f\"All CSV files have been compressed into {zip_filename}\")\n",
        "\n",
        "\n",
        "# URLs to scrape\n",
        "urls = [\n",
        "    \"https://www.demandsage.com/iphone-user-statistics/\",\n",
        "    \"https://backlinko.com/apple-statistics\"\n",
        "]\n",
        "\n",
        "all_csv_files = []\n",
        "\n",
        "for url in urls:\n",
        "    print(f\"\\nScraping tables from: {url}\")\n",
        "    csv_files = scrape_tables(url)\n",
        "    all_csv_files.extend(csv_files)\n",
        "\n",
        "if all_csv_files:\n",
        "    zip_filename = \"scraped_tables.zip\"\n",
        "    create_zip(all_csv_files, zip_filename)\n",
        "else:\n",
        "    print(\"No tables were scraped.\")\n",
        "\n",
        "print(\"Scraping completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGR0M-N_bf8",
        "outputId": "68977726-d356-43f1-db3c-854d2d1553c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping tables from: https://www.demandsage.com/iphone-user-statistics/\n",
            "Total tables found: 17\n",
            "Tables with specified classes: 17\n",
            "Data saved to How Many iPhone Users Are There In The World_.csv\n",
            "Data saved to Number Of iPhone Users In The United States.csv\n",
            "Data saved to Share Of iPhone Users Among Other Smartphone Users In The US.csv\n",
            "Data saved to iPhone Sales And Shipment Statistics.csv\n",
            "Data saved to iPhone Sales And Shipment Statistics.csv\n",
            "Data saved to iPhone Market Share Statistics.csv\n",
            "Data saved to iPhone Market Share Statistics.csv\n",
            "Data saved to iPhone Users Demographic.csv\n",
            "Data saved to iPhone Vs. Android.csv\n",
            "Data saved to iPhone Vs. Android.csv\n",
            "Data saved to Most Popular Apps on iPhone.csv\n",
            "Data saved to Most Popular Apps on iPhone.csv\n",
            "Data saved to Apple_s iPhone Revenue Statistics.csv\n",
            "Data saved to Apple_s iPhone Revenue Statistics.csv\n",
            "Data saved to iOS Market Share Statistics.csv\n",
            "Data saved to iOS Market Share Statistics.csv\n",
            "Data saved to It Is Predicted That The iPhone 15 Pro Max Will Be The Most-Sold iPhone.csv\n",
            "\n",
            "Scraping tables from: https://backlinko.com/apple-statistics\n",
            "Total tables found: 20\n",
            "Tables with specified classes: 20\n",
            "Data saved to Apple Annual Revenue.csv\n",
            "Data saved to Apple Quarterly Revenue.csv\n",
            "Data saved to Apple Revenue by Category.csv\n",
            "Data saved to Apple Revenue by Category.csv\n",
            "Data saved to Apple Revenue by Category.csv\n",
            "Data saved to Apple Revenue by Category.csv\n",
            "Data saved to Apple Revenue by Category.csv\n",
            "Data saved to Apple Revenue by Region.csv\n",
            "Data saved to Apple Revenue by Region.csv\n",
            "Data saved to Apple Revenue by Region.csv\n",
            "Data saved to Apple Revenue by Region.csv\n",
            "Data saved to Apple Revenue by Region.csv\n",
            "Data saved to Apple iPhone Market Share.csv\n",
            "Data saved to Apple iPad Market Share.csv\n",
            "Data saved to Number of Active Apple Devices Worldwide.csv\n",
            "Data saved to Apple Ad Revenue.csv\n",
            "Data saved to Apple Music Subscribers.csv\n",
            "Data saved to Number of Available Apps in the Apple App Store.csv\n",
            "Data saved to Number of Available Games in the Apple App Store.csv\n",
            "Data saved to How Many People Work at Apple_.csv\n",
            "Warning: File iPhone Sales And Shipment Statistics.csv not found and will be skipped.\n",
            "Warning: File iPhone Market Share Statistics.csv not found and will be skipped.\n",
            "Warning: File iPhone Vs. Android.csv not found and will be skipped.\n",
            "Warning: File Most Popular Apps on iPhone.csv not found and will be skipped.\n",
            "Warning: File Apple_s iPhone Revenue Statistics.csv not found and will be skipped.\n",
            "Warning: File iOS Market Share Statistics.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Category.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Category.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Category.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Category.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Region.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Region.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Region.csv not found and will be skipped.\n",
            "Warning: File Apple Revenue by Region.csv not found and will be skipped.\n",
            "All CSV files have been compressed into scraped_tables.zip\n",
            "Scraping completed.\n"
          ]
        }
      ]
    }
  ]
}